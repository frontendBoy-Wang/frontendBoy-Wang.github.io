{"title":"python爬虫超清桌面壁纸","uid":"fe95ebe795da542cb5396a487283045c","slug":"python/python爬虫超清桌面壁纸","date":"2023-03-13T02:46:35.000Z","updated":"2024-12-25T06:40:06.000Z","comments":true,"path":"api/articles/python/python爬虫超清桌面壁纸.json","keywords":"前端少年汪,前端,后端,全栈工程师,程序员,vue,react,Golang,Java,JavaScript,Python,frontendBoy-Wang","cover":"../img/33.png","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>这篇文章，来爬好看的电脑壁纸。还是使用python来进行爬虫。感觉使用python爬很方便。为什么又写爬虫呢，因为我又发现了一个好看的免费的<a href=\"https://desk.3gbizhi.com/\">壁纸网站</a>。</p></blockquote>\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e0f8e6f7a84245159f9394a9abf840fe~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=3010&h=1960&s=4130081&e=png&a=1&b=1f1f1f\" alt=\"image.png\"></p>\n<h1 id=\"目标网站：https-desk-3gbizhi-com\"><a href=\"#目标网站：https-desk-3gbizhi-com\" class=\"headerlink\" title=\"目标网站：https://desk.3gbizhi.com/\"></a>目标网站：<a href=\"https://desk.3gbizhi.com/\">https://desk.3gbizhi.com/</a></h1><p>我是打算获取首页18种类型的所有壁纸。</p>\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2386a0d73a3541b595db2f6e180615e8~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=2620&h=342&s=87643&e=png&a=1&b=232323\" alt=\"image.png\"></p>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><ul>\n<li>获取18种类型的url和图片总数量</li>\n<li>处理分页，遍历获取，遍历18个url下的每张缩略图片的详情url</li>\n<li>获取图片详情页面的图片的url或者下载链接</li>\n</ul>\n<p><img src=\"https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bf1bb8f2674244528bd49097c0d27b59~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=3010&h=1960&s=3513722&e=png&a=1&b=202020\" alt=\"image.png\"></p>\n<h1 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h1><h2 id=\"获取18种类型的url和图片总数量\"><a href=\"#获取18种类型的url和图片总数量\" class=\"headerlink\" title=\"获取18种类型的url和图片总数量\"></a>获取18种类型的url和图片总数量</h2><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"> \nimport requests\nfrom bs4 import BeautifulSoup\n\nheaders &#x3D; &#123;\n    &#39;user-agent&#39;: &#39;Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) &#39;\n                  &#39;Chrome&#x2F;106.0.0.0 Safari&#x2F;537.36 &#39;\n&#125;\n\n\n# 获取18种类型的url和图片总数量\ndef getHome(home_url):\n    res &#x3D; requests.get(home_url, headers&#x3D;headers).text\n    soup &#x3D; BeautifulSoup(res, &#39;html.parser&#39;)\n    type_item &#x3D; soup.find(class_&#x3D;&#39;r&#39;).find_all(&#39;a&#39;)\n\n    pic_list &#x3D; []  # 18种壁纸类型的url和数量\n    for (i, element) in enumerate(type_item):\n        string &#x3D; element.text\n        parts &#x3D; string.strip().split(&#39;\\n&#39;)\n        if len(parts) &#x3D;&#x3D; 2:\n            name, count &#x3D; parts[0], parts[1].strip(&#39;()张&#39;)\n            pic_list.append(&#123;\n                &quot;href&quot;: element.get(&#39;href&#39;),\n                &quot;type&quot;: name,\n                &quot;total_num&quot;: count,\n            &#125;)\n    print(pic_list)\n    return pic_list\n\n\nif __name__ &#x3D;&#x3D; &#39;__main__&#39;:\n    base_url &#x3D; &#39;https:&#x2F;&#x2F;desk.3gbizhi.com&#x2F;&#39;\n    getHome(base_url)</code></pre>\n\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/52cfc41c57754855bc986e273b81f6a9~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=3008&h=1960&s=843768&e=png&a=1&b=2f323e\" alt=\"image.png\"></p>\n<h2 id=\"遍历18个url获取每张缩略图片的详情url\"><a href=\"#遍历18个url获取每张缩略图片的详情url\" class=\"headerlink\" title=\"遍历18个url获取每张缩略图片的详情url\"></a>遍历18个url获取每张缩略图片的详情url</h2><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">def getTypePage(type_list):\n    all &#x3D; []\n    for (i, item) in enumerate(type_list):\n        try:\n            # if i &#x3D;&#x3D; 1:\n                type_detail_url &#x3D; []\n                print(i, item[&#39;type&#39;])\n                page &#x3D; int(item.get(&#39;total_num&#39;) &#x2F; 24) + 1\n                for pageItem in range(1, page + 1):\n                    print(&#39;第一页&#39;, pageItem)\n                    res &#x3D; requests.get(item.get(&#39;href&#39;) + &#39;&#x2F;index_&#123;&#125;.html&#39;.format(pageItem), headers&#x3D;headers).text\n                    soup &#x3D; BeautifulSoup(res, &#39;html.parser&#39;)\n                    print(item.get(&#39;href&#39;) + &#39;&#x2F;index_&#123;&#125;.html&#39;.format(pageItem))\n                    # 明星壁纸\n                    detail_url_list &#x3D; soup.find_all(class_&#x3D;&#39;box_black&#39;)\n                    for (vi, v) in enumerate(detail_url_list):\n                        pic_det_url &#x3D; v.find(class_&#x3D;&#39;desk imgw&#39;).get(&#39;href&#39;)\n                        pic_det_title &#x3D; v.find(class_&#x3D;&#39;title&#39;).text\n                        # print(vi, pic_det_url, pic_det_title)\n                        type_detail_url.append(&#123;\n                            &#39;url&#39;: pic_det_url,\n                            &#39;title&#39;: pic_det_title,\n                        &#125;)\n                print(len(type_detail_url))\n                all.append(&#123;item[&#39;type&#39;]: type_detail_url&#125;)\n\n        except Exception as e:\n            print(e)\n    print(all)</code></pre>\n\n<p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/97aeb058ed5a419eb1f18b4f45a69f8a~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=3008&h=1960&s=1086966&e=png&a=1&b=2d2f3c\" alt=\"image.png\"></p>\n<h2 id=\"获取详情页面里面的图片\"><a href=\"#获取详情页面里面的图片\" class=\"headerlink\" title=\"获取详情页面里面的图片\"></a>获取详情页面里面的图片</h2><p><img src=\"https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/74400d84f6b74c9eb187f57f8e69f4af~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=3094&h=2034&s=2997236&e=png&a=1&b=2f323e\" alt=\"image.png\"></p>\n<p>先说思路，把获取到的18种类型的缩略图遍历，先遍历类型再遍历类型里面的缩略图。获取图片详情里面的图片的src地址即可，然后调用写好的图片下载方法就可以把图片下载到本地了</p>\n<p>直接上代码</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">def getDetail(all):\n    for (i, item) in enumerate(all):\n        for (son, el) in enumerate(item):\n            try:\n                res &#x3D; requests.get(el.get(&#39;url&#39;), headers&#x3D;headers).text\n                soup &#x3D; BeautifulSoup(res, &#39;html.parser&#39;).find(class_&#x3D;&#39;wallphotos&#39;).find(&#39;img&#39;).get(&#39;src&#39;)\n                print(&#39;第&#123;&#125;第&#123;&#125;张开始下载...&#39;.format(i+1, son), soup)\n                download_img(soup,son)\n            except Exception as e:\n                print(e)</code></pre>\n\n\n<p>以上就是，整个的壁纸爬虫过程了，代码不过百行，是不是感觉很简单。不过我爬取的不是真正的原图，哈哈哈偷了一下懒，真正的原图要获取下载按钮里面的下载链接。这个网站做了一些反扒机制，要获取原图链接是需要点击下载按钮之后才会返回，而且还需要登录。这个稍微麻烦一些，我下期再介绍如何反扒，欲知后事如何，请听下回分解…</p>\n<p>最后给出全部完整代码</p>\n<h1 id=\"完整代码\"><a href=\"#完整代码\" class=\"headerlink\" title=\"完整代码\"></a>完整代码</h1><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">import os\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nheaders &#x3D; &#123;\n    &#39;user-agent&#39;: &#39;Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) &#39;\n                  &#39;Chrome&#x2F;106.0.0.0 Safari&#x2F;537.36 &#39;\n&#125;\n# 存放下载图片的文件夹名称\nnew_folder &#x3D; &#39;img&#39;\n# 获取当前目录\ncurrent_dir &#x3D; os.getcwd()\n\n# 新文件夹的名称\n# new_folder &#x3D; &#39;img_&#123;&#125;&#39;.format(type_list[type_index])  # 创建新文件夹\nif not os.path.exists(new_folder):\n    os.mkdir(os.path.join(current_dir, new_folder))\n\n\ndef download_img(img_src, i):\n    &quot;&quot;&quot;\n    图片下载\n    :param img_src: 图片的src\n    :param i: 序号\n    :return: void\n    &quot;&quot;&quot;\n    with open(&#39;.&#x2F;&#123;&#125;&#x2F;&#123;&#125;-&#123;&#125;&#39;.format(new_folder, i, img_src.split(&#39;&#x2F;&#39;)[-1]), &#39;wb&#39;) as f:\n        f.write(requests.get(img_src).content)\n    print(i, img_src, &#39;下载完成&#39;)\n\n\ndef get_soup(url):\n    return BeautifulSoup(requests.get(url, headers&#x3D;headers).text, &#39;html.parser&#39;)\n\n\n# 获取18种类型的url和图片总数量\ndef getHome(home_url):\n    try:\n        res &#x3D; requests.get(home_url, headers&#x3D;headers).text\n        soup &#x3D; BeautifulSoup(res, &#39;html.parser&#39;)\n        type_item &#x3D; soup.find(class_&#x3D;&#39;r&#39;).find_all(&#39;a&#39;)\n\n        pic_list &#x3D; []  # 18种壁纸类型的url和数量\n        for (i, element) in enumerate(type_item):\n            string &#x3D; element.text\n            parts &#x3D; string.strip().split(&#39;\\n&#39;)\n            if len(parts) &#x3D;&#x3D; 2:\n                name, count &#x3D; parts[0], parts[1].strip(&#39;()张&#39;)\n                pic_list.append(&#123;\n                    &quot;href&quot;: element.get(&#39;href&#39;),\n                    &quot;type&quot;: name,\n                    &quot;total_num&quot;: int(count),\n                &#125;)\n        # print(pic_list[0])\n        return pic_list\n    except Exception as e:\n        print(e)\n\n\ndef getTypePage(type_list):\n    all &#x3D; []\n    for (i, item) in enumerate(type_list):\n        try:\n            if i &#x3D;&#x3D; 1:\n                type_detail_url &#x3D; []\n                print(i, item[&#39;type&#39;])\n                page &#x3D; int(item.get(&#39;total_num&#39;) &#x2F; 24) + 1\n                for pageItem in range(1, page + 1):\n                    print(&#39;第一页&#39;, pageItem)\n                    res &#x3D; requests.get(item.get(&#39;href&#39;) + &#39;&#x2F;index_&#123;&#125;.html&#39;.format(pageItem), headers&#x3D;headers).text\n                    soup &#x3D; BeautifulSoup(res, &#39;html.parser&#39;)\n                    print(item.get(&#39;href&#39;) + &#39;&#x2F;index_&#123;&#125;.html&#39;.format(pageItem))\n                    # 明星壁纸\n                    detail_url_list &#x3D; soup.find_all(class_&#x3D;&#39;box_black&#39;)\n                    for (vi, v) in enumerate(detail_url_list):\n                        pic_det_url &#x3D; v.find(class_&#x3D;&#39;desk imgw&#39;).get(&#39;href&#39;)\n                        pic_det_title &#x3D; v.find(class_&#x3D;&#39;title&#39;).text\n                        # print(vi, pic_det_url, pic_det_title)\n                        type_detail_url.append(&#123;\n                            &#39;url&#39;: pic_det_url,\n                            &#39;title&#39;: pic_det_title,\n                        &#125;)\n                # print(len(type_detail_url))\n                # all.append(&#123;item[&#39;type&#39;]: type_detail_url&#125;)\n                all.append(type_detail_url)\n\n        except Exception as e:\n            print(e)\n    # print(all)\n    return all\n\n\ndef getDetail(all):\n    for (i, item) in enumerate(all):\n        for (son, el) in enumerate(item):\n            try:\n                res &#x3D; requests.get(el.get(&#39;url&#39;), headers&#x3D;headers).text\n                soup &#x3D; BeautifulSoup(res, &#39;html.parser&#39;).find(class_&#x3D;&#39;wallphotos&#39;).find(&#39;img&#39;).get(&#39;src&#39;)\n                print(&#39;第&#123;&#125;页第&#123;&#125;张开始下载...&#39;.format(i + 1, son), soup)\n                download_img(soup, son)\n            except Exception as e:\n                print(e)\n\n\nif __name__ &#x3D;&#x3D; &#39;__main__&#39;:\n    base_url &#x3D; &#39;https:&#x2F;&#x2F;desk.3gbizhi.com&#x2F;&#39;\n    pic_lists &#x3D; getHome(base_url)\n    all1 &#x3D; getTypePage(pic_lists)\n    getDetail(all1)</code></pre>\n\n<p>放几张壁纸给大家看看，欣赏一下(虽软不是原图)</p>\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2172eaf3a9404aca89c5441e59c0badf~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=2664&h=1794&s=2273454&e=png&a=1&b=282627\" alt=\"image.png\"></p>\n<p><img src=\"https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/af2b20b739f0479e9529e1af30d5fa42~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=1280&h=720&s=140408&e=jpg&b=695237\" alt=\"0-63c089d87be51251c45a784fcc90d8bf.jpg\"></p>\n<p><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e9135507b93c4f9bb904f98c219690d0~tplv-k3u1fbpfcp-jj-mark:0:0:0:0:q75.image#?w=1280&h=720&s=120388&e=jpg&b=ca9d70\" alt=\"3-c71aadd1491e3131b3d67dfb7ae3439b.jpg\"></p>\n","feature":true,"text":"前言 这篇文章，来爬好看的电脑壁纸。还是使用python来进行爬虫。感觉使用python爬很方便。为什么又写爬虫呢，因为我又发现了一个好看的免费的壁纸网站。 目标网站：https://desk.3gbizhi.com/我是打算获取首页18种类型的所有壁纸。 思路 获取18种类型的...","link":"","photos":[],"count_time":{"symbolsCount":"8.3k","symbolsTime":"8 mins."},"categories":[{"name":"python","slug":"python","count":2,"path":"api/categories/python.json"}],"tags":[{"name":"爬虫","slug":"爬虫","count":2,"path":"api/tags/爬虫.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%89%8D%E8%A8%80\"><span class=\"toc-text\">前言</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%9B%AE%E6%A0%87%E7%BD%91%E7%AB%99%EF%BC%9Ahttps-desk-3gbizhi-com\"><span class=\"toc-text\">目标网站：https:&#x2F;&#x2F;desk.3gbizhi.com&#x2F;</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%80%9D%E8%B7%AF\"><span class=\"toc-text\">思路</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E4%BB%A3%E7%A0%81\"><span class=\"toc-text\">代码</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%8E%B7%E5%8F%9618%E7%A7%8D%E7%B1%BB%E5%9E%8B%E7%9A%84url%E5%92%8C%E5%9B%BE%E7%89%87%E6%80%BB%E6%95%B0%E9%87%8F\"><span class=\"toc-text\">获取18种类型的url和图片总数量</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E9%81%8D%E5%8E%8618%E4%B8%AAurl%E8%8E%B7%E5%8F%96%E6%AF%8F%E5%BC%A0%E7%BC%A9%E7%95%A5%E5%9B%BE%E7%89%87%E7%9A%84%E8%AF%A6%E6%83%85url\"><span class=\"toc-text\">遍历18个url获取每张缩略图片的详情url</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%8E%B7%E5%8F%96%E8%AF%A6%E6%83%85%E9%A1%B5%E9%9D%A2%E9%87%8C%E9%9D%A2%E7%9A%84%E5%9B%BE%E7%89%87\"><span class=\"toc-text\">获取详情页面里面的图片</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81\"><span class=\"toc-text\">完整代码</span></a></li></ol>","author":{"name":"前端少年汪","slug":"blog-author","avatar":"https://p3-passport.byteimg.com/img/user-avatar/46cf67c329246db441271a06f6132633~100x100.awebp","link":"/","description":"一个追逐技术的全栈工程师 <br /> @ <b>公众号：前端少年汪</b>","socials":{"github":"https://github.com/frontendBoy-Wang","twitter":"","stackoverflow":"https://stackoverflow.com/users/16856904/frontendboy-wang","wechat":"","qq":"","weibo":"","zhihu":"https://www.zhihu.com/people/qian-ke-sao-ren-wmq","csdn":"https://blog.csdn.net/qq_44647871?spm=1010.2135.3001.5421","juejin":"https://juejin.cn/user/3570847174897447","customs":{"bilibili":{"icon":"/svg/bilibili.svg","link":"https://space.bilibili.com/392982262"}}}},"mapped":true,"prev_post":{"title":"VUE3新特性","uid":"76203a7a58be01e7fd00254c86e3c923","slug":"vue/再遇vue之vue3新特性","date":"2023-03-15T02:46:35.000Z","updated":"2024-12-25T06:40:06.000Z","comments":true,"path":"api/articles/vue/再遇vue之vue3新特性.json","keywords":"前端少年汪,前端,后端,全栈工程师,程序员,vue,react,Golang,Java,JavaScript,Python,frontendBoy-Wang","cover":"../img/66.jpeg","text":" 想起来上次好好认真学vue，还是刚实习那会儿，如今回头看，已是三年有余了。vue从当初的vue2也大升级到vue3了。新的 API，新的语法糖，新的响应式…如今，我已不是以前那个小白了，对vue和js的使用也越来越熟练了，打算在好好系统的复习一下vue3的新特性。 vue2和v...","link":"","photos":[],"count_time":{"symbolsCount":"9.6k","symbolsTime":"9 mins."},"categories":[{"name":"vue","slug":"vue","count":2,"path":"api/categories/vue.json"}],"tags":[{"name":"vue","slug":"vue","count":2,"path":"api/tags/vue.json"}],"author":{"name":"前端少年汪","slug":"blog-author","avatar":"https://p3-passport.byteimg.com/img/user-avatar/46cf67c329246db441271a06f6132633~100x100.awebp","link":"/","description":"一个追逐技术的全栈工程师 <br /> @ <b>公众号：前端少年汪</b>","socials":{"github":"https://github.com/frontendBoy-Wang","twitter":"","stackoverflow":"https://stackoverflow.com/users/16856904/frontendboy-wang","wechat":"","qq":"","weibo":"","zhihu":"https://www.zhihu.com/people/qian-ke-sao-ren-wmq","csdn":"https://blog.csdn.net/qq_44647871?spm=1010.2135.3001.5421","juejin":"https://juejin.cn/user/3570847174897447","customs":{"bilibili":{"icon":"/svg/bilibili.svg","link":"https://space.bilibili.com/392982262"}}}},"feature":false},"next_post":{"title":"Gin入门教程：从零开始学习Go语言Web框架","uid":"3140e76ccb52a413540ef6b8d667bcb4","slug":"go/Gin入门教程：从零开始学习Go语言Web框架","date":"2023-03-01T02:14:26.000Z","updated":"2024-12-25T06:40:06.000Z","comments":true,"path":"api/articles/go/Gin入门教程：从零开始学习Go语言Web框架.json","keywords":"前端少年汪,前端,后端,全栈工程师,程序员,vue,react,Golang,Java,JavaScript,Python,frontendBoy-Wang","cover":"../img/10.png","text":" 在Go语言的Web开发领域，Gin框架无疑是一个备受关注的轻量级框架。它具有快速、高效、易用等特点，非常适合用于构建Web应用程序。本篇博客将带领大家从零开始学习Gin框架，包括安装、基本用法和常用功能等内容。Gin 是一个用 Go 语言编写的 Web 框架，它提供了快速构建高...","link":"","photos":[],"count_time":{"symbolsCount":"10k","symbolsTime":"9 mins."},"categories":[{"name":"go","slug":"go","count":3,"path":"api/categories/go.json"}],"tags":[{"name":"gin go","slug":"gin-go","count":1,"path":"api/tags/gin-go.json"}],"author":{"name":"前端少年汪","slug":"blog-author","avatar":"https://p3-passport.byteimg.com/img/user-avatar/46cf67c329246db441271a06f6132633~100x100.awebp","link":"/","description":"一个追逐技术的全栈工程师 <br /> @ <b>公众号：前端少年汪</b>","socials":{"github":"https://github.com/frontendBoy-Wang","twitter":"","stackoverflow":"https://stackoverflow.com/users/16856904/frontendboy-wang","wechat":"","qq":"","weibo":"","zhihu":"https://www.zhihu.com/people/qian-ke-sao-ren-wmq","csdn":"https://blog.csdn.net/qq_44647871?spm=1010.2135.3001.5421","juejin":"https://juejin.cn/user/3570847174897447","customs":{"bilibili":{"icon":"/svg/bilibili.svg","link":"https://space.bilibili.com/392982262"}}}},"feature":true}}